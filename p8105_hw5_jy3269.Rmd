---
title: "p8105_hw5_jy3269"
author: "Jingyi Yao"
date: "`r Sys.Date()`"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(gtsummary)
library(p8105.datasets)
set.seed(1)
```

# Problem 1

```{r,message=FALSE,warning=FALSE}
full_df = 
  tibble(
    files = list.files("data/zip_data/"),
    path = str_c("data/zip_data/", files)
  ) %>% 
  mutate(data = map(path, read_csv)) %>% 
  unnest()
```


```{r}
tidy_df = 
  full_df %>% 
  mutate(
    files = str_replace(files, ".csv", ""),
    group = str_sub(files, 1, 3)) %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    values_to = "outcome",
    names_prefix = "week_") %>% 
  mutate(week = as.numeric(week)) %>% 
  select(group, subj = files, week, outcome)
```



```{r}
tidy_df %>% 
  ggplot(aes(x = week, y = outcome, group = subj, color = group)) + 
  geom_point() + 
  geom_path() + 
  facet_grid(~group)
```

This plot suggests high within-subject correlation -- subjects who start above average end up above average, and those that start below average end up below average. Subjects in the control group generally don't change over time, but those in the experiment group increase their outcome in a roughly linear way. 






# Problem 2

### 1. Describe the raw data
```{r}
raw_data <- read_csv("./data/homicide-data.csv",show_col_types = FALSE)

dim(raw_data)

colnames(raw_data)

```

\ \par

1. The raw data set contains `r nrow(raw_data)` rows of observations (homicide) and `r ncol(raw_data)` columns of variables (homicide features).

2. The variables are `r colnames(raw_data)`

3. This data set mainly documents the homicides in 50 large U.S. cities. Each row is a documented homicide, including the victim's information, the place where the homicide took place and the result.


\ \par
### 2. Create a `city_state` variable
```{r}
homicide <- raw_data %>% 
  mutate(city_state = str_c(city,", ",state)) %>% 
  select(uid,city_state,everything())   # put the newly created variable at the front

homicide %>%
  head() %>% 
  knitr::kable()   # show the first 6 rows of the homicide dataset

```

\ \par
### 3. Summarize within cities
```{r}
homicide %>% 
  group_by(city_state) %>% 
  summarize(
    total_homicides = n(),
    unsolved_homicides = sum(disposition == "Open/No arrest") + sum(disposition == "Closed without arrest")) %>% 
  arrange(desc(total_homicides)) %>%   # arranged by total_homicides counts
  knitr::kable()

```

\ \par

From the table we know that :

1. Chicago has the most total homicides (5535) and the most unsolved homicides(4073).

2. Tulsa in Alabama has only 1 case of homicide and 0 unsolved homicide.


\ \par
### 4. Focus on Baltimore, MD
```{r}
baltimore <- homicide %>% 
  filter(city_state == "Baltimore, MD") %>% 
  mutate(
    unsolved_homicides = ifelse(disposition == "Open/No arrest" | disposition == "Closed without arrest", 1,0))  # create a binary column showing the status

```



\ \par
### 5. Conduct proportion test and save the results
```{r}
result_baltimore <- prop.test(sum(baltimore$unsolved_homicides),length(baltimore$unsolved_homicides)) %>% 
  broom::tidy()   # tidy the result

result_baltimore

# the result is save in the result folder in the R project
save(result_baltimore, file = "result/results_baltimore.RData")
```
\ \par

The `prop.test` on Baltimore estimates that 64.56% of the homicides are unsolved, and the 95% confidence interval for the proportion estimate is ( 0.6275625, 0.6631599	) 


\ \par
### 6. Pull the estimated proportion and confidence intervals
```{r}
# using pull()
prop_estimate_baltimore <- result_baltimore %>% 
  pull(estimate)

lower_bound_baltimore <- result_baltimore %>% 
  pull(conf.low)

upper_bound_baltimore <- result_baltimore %>% 
  pull(conf.high)

baltimore_estimate <- list(
  "proportion_estimate" = prop_estimate_baltimore,
  "lower_CI_estimate" = lower_bound_baltimore,
  "upper_CI_estimate" = upper_bound_baltimore
) %>% 
  bind_rows()

baltimore_estimate %>% knitr::kable()

```


```{r}
# not using pull(), using select()
result_baltimore <- prop.test(sum(baltimore$unsolved_homicides),length(baltimore$unsolved_homicides)) %>% 
  broom::tidy() %>% 
  select(estimate,conf.low,conf.high)

result_baltimore %>% knitr::kable()

```


### 7. define a function
```{r}
unsolved <- function(citystate){
  # filter a city
  by_city <- homicide %>% 
    filter(city_state == citystate) %>% 
    mutate(unsolved_homicides = ifelse(disposition == "Open/No arrest" | disposition == "Closed without arrest", 1,0)) 
  
  # conduct prop test for the filtered city
    result <- prop.test(sum(by_city$unsolved_homicides),length(by_city$unsolved_homicides)) %>% 
    broom::tidy() %>% 
    select(estimate, conf.low, conf.high)
    
  # output
    result 
}
  
```

\ \par 

We define a function called `unsolved()` to get the `prop.test` result of the each city in the dataset.

The input of the function is the `city_state` name and the output is the proportion point estimate and 95% confidence interval of the estimate.


\ \par
### 8. iterate the function among each city
```{r}
cityname = unique(homicide$city_state) 

result <- expand_grid(city_state = cityname) %>% 
  mutate(prop_test_result = map(cityname, unsolved)) %>% 
  unnest(prop_test_result) %>% 
  arrange(desc(estimate))

result %>% knitr::kable(digits = 3)
```
\ \par

### 9. plot the result
```{r}
result %>%
  ggplot(aes(group = city_state, y = reorder(city_state, estimate))) + 
  geom_point(aes(x = estimate)) +
  geom_errorbar(aes(xmin = conf.low, xmax = conf.high)) +
  theme(axis.text.y = element_text(hjust = 0.5,size = 6)) +
  labs(y = "City,State", x = "Estimated Proportion and 95% CI",title = "Unsolved Homicides")

```

The x-axis is the estimated proportion of unsolved homicides and its 95% CI.

The y-axis is each city_state in the dataset.

We can tell from the plot that Chicago's unsolved homicides proportion estimate is the the highest and it is significantly higher than the rest.




\ \par
\ \par

## Problem 3

### 1. set the design element and generate 5000 samples 
```{r}
samples <- rerun(5000,rnorm(n = 30,mean = 0,sd = 5))

```

\ \par
### 2. define a function to t.test 1 sample and save the estimate and p-value
```{r}
t_test <- function(mu = 0) {
  sample <- tibble(rnorm(n = 30, mean = mu, sd = 5))
  
  result <- t.test(sample) %>% 
    broom::tidy() %>% 
    select(estimate,p.value)
  
  result
}
  
```

\ \par
### 3. use the defined function to t.test 5000 samples with the same mean = 0
```{r}
mean_0 <- expand_grid(mean = 0, iteration = 1:5000) %>% 
  mutate(result = map(mean,t_test)) %>% 
  unnest(result)

dim(mean_0)
head(mean_0)

```

### 4. t.test samples with different means
```{r}
mean_multi <- expand_grid(mean = 1:6, iteration = 1:5000) %>% 
  mutate(result = map(mean,t_test)) %>% 
  unnest(result)

dim(mean_multi)
head(mean_multi)

```

### 5. plot of power
```{r}
mean_multi %>%
  group_by(mean) %>% 
  summarize(proportion_rejected = sum(p.value < 0.05)/5000) %>% 
  ggplot(aes(x = mean,y = proportion_rejected)) +
  scale_x_continuous(limits = c(1,6), breaks = seq(1,6,1)) + 
  geom_point() + geom_path() +
  labs(x = "True Mean",y = "Power ( proportion of rejection )",title = "Power of t.test vs. Different Means")

```

 \ \par
 
 From the plot we know that :
 
 1. As the true mean increases, the proportion of rejection increases. 
 
 2. In other words, the power increases as the effect size increases, and ultimately approaches 1.

 3. The increasing rate (slope) of power decreases as the effect size increases.


\ \par
### 6. plot of estimated means
```{r,warning=FALSE}
mean_multi %>%
  group_by(mean) %>% 
  summarize(average_estimate = mean(estimate,na.rm = T)) %>% 
  ggplot(aes(x = mean,y = average_estimate)) +
  scale_x_continuous(limits = c(1,6), breaks = seq(1,6,1)) + 
  geom_point() + geom_path() +
  labs(x = "True Mean",y = "Average Estimate Mean",title = "Estimated Means")

```

### 7. plot all the estimates vs. the rejected estimates
```{r}
rejected_estimate <- mean_multi %>% 
  filter(p.value < 0.05) %>% group_by(mean) %>% 
  summarize(average_estimate = mean(estimate,na.rm = T)) %>% 
  ungroup()

full_estimate <- mean_multi %>% 
  group_by(mean) %>% 
  summarize(average_estimate = mean(estimate,na.rm = T)) %>% 
  ungroup()
  
ggplot(full_estimate,aes(x = mean, y = average_estimate)) +
  geom_line(data = full_estimate,aes(colour = "blue")) +
  geom_line(data = rejected_estimate,aes(colour = "red")) +
  scale_color_manual(name = " ", values = c("blue" = "blue", "red" = "red"),
                     labels = c('All Estimates','Rejected Estimates')) +
  geom_point(data = full_estimate,colour = "blue") +
  geom_point(data = rejected_estimate,colour = "red") +
  scale_x_continuous(limits = c(1,6), breaks = seq(1,6,1)) +
  labs(x = "True Mean",y = "Average Estimate Mean",title = "All vs. Rejected Estimates")
  
  

```

\ \par

From the plots above, we know :

1. When the effect size is small(less than 4 in this case), the sample average of mu hat when the null is rejected is different from the true value of mu. It is always larger than the true value of mu. This is because the effect size is relatively small and the power is relatively low.

2. When the effect size gets larger(larger or equal to 4 in this case), the sample average of mu hat when the null is rejected is approximately equal to the true value of mu. This is because the power is increasing as the effect size increases.


